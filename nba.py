# -*- coding: utf-8 -*-
"""nba.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gQhrZFenVHJgboEQO4T42i0roeawGYYF
"""
#Programming 2
#Inara Rupani
#ID: 1001534052

#This assignmnet is completed over google colab therefore we are able to use wget commnad to load the .csv file

!wget -O 2018-2019_NBA_Stats.csv https://uta.instructure.com/courses/61673/files/10505095/preview?verifier=1qoqyIxNj5MjnRrfNGQsUpWHAQs0TDZiyDLfTKps

#Adding Extra comments to how my file is working
#Tools and libraries
import io
import pandas as pd
from google.colab import files

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV
from sklearn.tree import export_graphviz

#Read from the csv file and return a Pandas DataFrame.
stats_file = '/content/2018-2019_NBA_Stats.csv'
nba = pd.read_csv(io.FileIO(stats_file))

print("Number of rows and number of columns:", nba.shape)
original_headers = list(nba.columns.values)
print("Column names:", original_headers)
print()

#"Position" is the class attribute/column we are predicting. 
class_column = 'Position'

#The dataset contains attributes such as player name and team name. 
#We know that they are not useful for classification and thus do not 
#include them as features.


#Modified the feature column used for data manipulation and analyais on the data set.
#Reomved: Games Played and Minutes Per game as they have no corerlation on defining the position of player in the game
feature_columns = ['Points Per Game', 'Total Rebounds Per Game',\
                   'Assists Per Game', 'Three Points Made Per Game',\
                   'Steals Per Game', 'Blocks Per Game']

#Pandas DataFrame allows you to select columns. 
#We use column selection to split the data into features and class. 

#getting the relevant nba features from our data using pandas dataframe
#Used to select specific columns 
nba_feature = nba[feature_columns]
nba_class = nba[class_column]

#Print nba data, its feature columns and class attribute, 
#Modified to print first five than three rows.
#Model classification is applied on the whole data set, but only for printing purposes we are printing to check how our data is formatted and looks.
print("First five rows")
print(nba[0:5])
print()
print("Feature columns, first five rows:")
print(nba_feature[0:5])
print()
print("Class column, first five rows:")
print(nba_class[0:5])
print()

#Split the dataset into training/test set.
train_feature, test_feature, train_class, test_class = \
    train_test_split(nba_feature, nba_class, stratify=nba_class, \
    train_size=0.75, test_size=0.25)

#model = KNeighborsClassifier(n_neighbors=5) 
#We implement all the other classification on our model. 
#All the functions that were tried and compared are written below but are commented out, except for the one I have finalized and works the best.

#Test 1
#model = DecisionTreeClassifier(max_depth=5, random_state=0)
#model = DecisionTreeClassifier(max_depth=7, random_state=0)

#Test 2 
#model=LinearSVC(dual=False,tol=1e-4,random_state=0,max_iter=2000)
#model=LinearSVC(dual=False,tol=1e-4,random_state=0,max_iter=5000)
model=LinearSVC(dual=False,tol=1e-4,random_state=0,max_iter=7000)

#Test 3
#model=GaussianNB()

#Test 4a - Regected as penalty=l1 cant be applied for our data set.
#model = LogisticRegression(penalty='l1',tol=1e-4,random_state=0,max_iter=7000)

#Test 4b 
#model = LogisticRegression(penalty='l2',tol=1e-4,random_state=0,max_iter=2000)
#model = LogisticRegression(penalty='l2',tol=1e-4,random_state=0,max_iter=5000)
#model = LogisticRegression(penalty='l2',tol=1e-4,random_state=0,max_iter=7000)

#each run of classification model usese these to print us the classification analysis. 
#modified to use cv=10 as most data sets use the 10 as default value.
scores = cross_val_score(model, nba_feature, nba_class, cv=10)

#Print 10 fold cros-svalidation scores and its average
print("Cross-validation scores: {}".format(scores))
print("Average cross-validation score: {:.2f}".format(scores.mean()))
print()
#Train our training set to test on set data set.
model.fit(train_feature, train_class)
print("Test set accuracy: {:.2f}".format(model.score(test_feature, test_class)))
print()
#Make test set predition based on training we performed on our training data set. 
prediction = model.predict(test_feature)
print("Test set predictions:\n{}".format(prediction))
print('')
#Create the confusion matrix for our data using pandas "crosstab" feature.
print("Confusion matrix:")
print(pd.crosstab(test_class, prediction, rownames=['True'], colnames=['Predicted'], margins=True))
print()
print("Classification report:")
print(classification_report(test_class, prediction))

#Save the training and test sets into CSV files.
train_class_df = pd.DataFrame(train_class, columns=[class_column])     
train_data_df = pd.merge(train_class_df, train_feature, left_index=True, right_index=True)
train_data_df.to_csv('train_data.csv', float_format='%.3f', index=False)

temp_df = pd.DataFrame(test_class, columns=[class_column])
temp_df['Predicted Pos']=pd.Series(prediction, index=temp_df.index)
test_data_df = pd.merge(temp_df, test_feature, left_index=True, right_index=True)
test_data_df.to_csv('test_data.csv', float_format='%.3f', index=False)

